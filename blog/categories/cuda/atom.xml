<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Cuda | Curious Explorations]]></title>
  <link href="http://kmmankad.github.io/blog/categories/cuda/atom.xml" rel="self"/>
  <link href="http://kmmankad.github.io/"/>
  <updated>2016-04-04T22:13:24+05:30</updated>
  <id>http://kmmankad.github.io/</id>
  <author>
    <name><![CDATA[kmmankad]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[OpenACC: Analyze, Express, Tweak!]]></title>
    <link href="http://kmmankad.github.io/blog/2016/04/03/openacc-analyze/"/>
    <updated>2016-04-03T11:22:49+05:30</updated>
    <id>http://kmmankad.github.io/blog/2016/04/03/openacc-analyze</id>
    <content type="html"><![CDATA[<h2>Whats OpenACC?</h2>

<p>From <a href="http://developer.nvidia.com/openacc">http://developer.nvidia.com/openacc</a>:</p>

<blockquote><p>OpenACC is a directive-based programming model designed to provide a simple yet powerful approach to accelerators without significant programming effort.</p></blockquote>

<p>What that is means is, you can pickup existing code written for an x86 CPU, and add some compiler <code>#pragmas</code>, compile with an OpenACC capable compiler - and voila! You get accelerated binaries for a range of hardware accelerators - Nvidia GPUs, AMD GPUs and even Intel multi-core CPUs. Thats really the USP of OpenACC - a single copy of the source code will deliver performance portability across this range of hardware platforms.
So, to be successful with OpenACC all you need are strong concepts in parallel programming, some know-how about OpenACC syntax and you’re good to go! You dont need to really know too many lower level hardware details with OpenACC, as opposed to, maybe CUDA C. However, this is a double edged sword - I will revisit this later in this post.</p>

<p>There are some really good tutorials on OpenACC itself available online:<br/>
1. <a href="https://devblogs.nvidia.com/parallelforall/getting-started-openacc/">Jeff Larkin&rsquo;s post on the Parallel Forall blog</a><br/>
2. Jeff Larkin&rsquo;s sessions from GTC 2013 - recordings on Youtube here : <a href="https://www.youtube.com/watch?v=0e5TiwZd_wE">Part1</a> <a href="https://www.youtube.com/watch?v=YueszvniRUE">Part2</a></p>

<p>The recommended approach for parallelism anywhere is to:<br/>
1. Try and use existing parallel optimized libraries like cuBLAS, cuDNN etc. if they exist for your application.<br/>
2. If you dont get those, try OpenACC on your code. That should get you about 80% of the maximum available performance.<br/>
<em>Ofcourse, that is a very rough number and is subject to, you guessed it, your code and the GPU hardware you&rsquo;re running.</em>
3. Roll your own CUDA kernels. This is definitely the most involved of the 3 options, but it will allow you to squeeze
every last drop of that good perf juice from your software and hardware.</p>

<p>OpenACC tutorials online often use the Jacobi Iteration/sAXPY example to demonstrate OpenACC, but all that those examples teach us are syntax constructs. However, if you use OpenACC in the real world, you’ll know it&rsquo;s all about how you analyze your source code, understand its scope for parallelism and finally express that formally via OpenACC syntax. What this post is really about is about the analysis of a simple program, which is hopefully a little less trivial than the Jacobi type examples all over the net.</p>

<p>First off, some logistics about tool installation and setup.</p>

<ul>
<li>We will be using the PGI Compiler today, which you can get from the <a href="http://www.pgroup.com/support/download_pgi2016.php?view=current">PGroup&rsquo;s site</a></li>
<li>You can also download the <a href="https://developer.nvidia.com/openacc-toolkit">OpenACC toolkit from NVIDIA</a></li>
</ul>


<p>If you have everything correctly setup, try <code>pgcc --version</code> as shown below
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>PGI Workstation 15.10 <span class="o">(</span>64<span class="o">)</span>
</span><span class='line'>PGI<span class="nv">$ </span>pgcc <span class="p">&amp;</span>ndash<span class="p">;</span>version&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;pgcc 15.10-0 64-bit target on x86-64 Windows -tp haswell
</span><span class='line'>The Portland Group - PGI Compilers and Tools
</span><span class='line'>Copyright <span class="p">&amp;</span>copy<span class="p">;</span> 2015, NVIDIA CORPORATION.  All rights reserved.
</span></code></pre></td></tr></table></div></figure></p>

<p>Now, onto our target today - a subroutine that converts a hexadecimal string to base64. I picked this up from the <a href="http://cryptopals.com/">matasano cryptography challenges</a> I&rsquo;m attempting on the side and decided it&rsquo;d be a good example for this tutorial.</p>

<p>Heres a brief overview of the algorithm itself:
1. Take 3 bytes of input hex data at a time,
2. Do some bitwise concatenation (shift and OR) and get indexes of 4 base64 characters that these 3 bytes are encoded into
3. Lookup the actual base64 characters using these indices.
..and heres a quick diagram to explain that:</p>

<p><img src="images/openacc/ASCII_to_b64.PNG" title="Figure 1: Hex to Base64" alt="Diagram showing Hex to Base64 conversion" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[So You Want to CUDA?]]></title>
    <link href="http://kmmankad.github.io/blog/2016/02/29/so-you-want-to-cuda/"/>
    <updated>2016-02-29T19:12:59+05:30</updated>
    <id>http://kmmankad.github.io/blog/2016/02/29/so-you-want-to-cuda</id>
    <content type="html"><![CDATA[<p>This is a post about various available resources, and how you could go about becoming a real CUDA pro. This post isn&rsquo;t about convincing you about why you should definitely learn CUDA - I&rsquo;ll leave that to the voices in and around your head.</p>

<p>To start out, I would highly recommend going through the free MOOC from Udacity - <a href="https://www.udacity.com/course/intro-to-parallel-programming--cs344">Intro to Parallel Programming</a>. This is a course that isn&rsquo;t too technical right off the bat and yet its assignments are non-trivial and could also be a bit challenging for some. But they really help you get some real world exposure to parallel programming in general, apart from the CUDA specific knowledge you would gain in the process. The course really helps develop a &lsquo;think parallel&rsquo; mindset - which I feel is as important, (if not more) compared to the knowledge of the actual semantics of a specific programming language or platform. The best part? You can do this without any special hardware - its all in the cloud!</p>

<p>Along with the udacity course, there are a couple of great texts I would urge you guys to get:</p>

<ol>
<li><p><a href="http://www.amazon.com/CUDA-Example-Introduction-General-Purpose-Programming/dp/0131387685/ref=pd_bbs_sr_1/103-9839083-1501412?ie=UTF8&amp;s=books&amp;qid=1186428068&amp;sr=1-1">Sanders &amp; Kandrot. CUDA by Example: An Introduction to General-Purpose GPU Programming</a>
This first one is a good text for beginners because it presents a very approachable learning curve. It has lots of small code examples, something I personally like. It lives up to its title in that respect. Having digestible code examples allow you to tinker with different concepts till you get the hang of things, without the overhead of programming full assignments. The book&rsquo;s code is available for download on <a href="https://developer.nvidia.com/cuda-example">NVIDIA&rsquo;s site here</a> and serves as handy reference later on as well. However, this book does not go too deep into the application side, and the &lsquo;bigger picture&rsquo; of parallel programming. Thats where the next book is better.</p></li>
<li><p><a href="http://store.elsevier.com/Programming-Massively-Parallel-Processors/David-Kirk/isbn-9780124159921/">Kirk &amp; Hwu. Programming Massively Parallel Processors</a>
This book definitely dives a bit deeper with regard to the technical aspects. Since it was created keeping in mind a typical graduate-level course on this subject, each chapter has exercises as well. Chapter 6 on performance considerations, and Chapter 7 on floating point math are two I consider particularly important for a learner to understand early on. The chapters on computational thinking and OpenCL make this a complete text on parallel programming. In addition, the code for the case studies discussed has been made available <a href="http://www.ks.uiuc.edu/Research/vmd/projects/ece498/">freely available online.</a></p></li>
</ol>


<p>And as you get more hands-on with the programming aspects of it, you will be able to appreciate the wealth of info in the <a href="http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html">CUDA Best Practices Guide</a>. I actually have a printed copy I refer to often.</p>

<p>Among must-read blogs, there is <a href="https://devblogs.nvidia.com/parallelforall/">NVIDIA&rsquo;s Parallel Forall blog</a> that has some really well written articles on a wide variety of topics and applications in accelerated computing. Most of the CUDA related content posted here is best understood by someone who already has a higher-than-basic understanding of CUDA. Still, do subscribe.</p>

<p>I almost forgot to mention the <a href="https://nvidia.qwiklab.com/">hands-on labs offered by NVIDIA via qwiklabs</a>. While these aren&rsquo;t anywhere as fully featured as the resources mentioned above, these serve as good exercises nonetheless. These are also in the cloud, hosted on <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using_cluster_computing.html">GPU enabled AWS instances</a>.</p>

<p>Though there are lots of such free(-ish) learning resources out there, you really need access to some hardware in order to really sharpen your skills. But this does not mean you need to spend big bucks. Lots of older GPUs support CUDA, and if you&rsquo;re part of an academic institution, you could also look at <a href="https://developer.nvidia.com/academic_hw_seeding">Nvidia&rsquo;s hardware grant program</a>. You can also run your CUDA code on your multicore CPU (coming-soon-a-link-to-a-tutorial-on-how-to-do-that)</p>

<p>And finally, you need to have a project that you really want to invest your sweat and skills into. Something to tie all of this together. It could be a cryptographic algorithm, or a massively parallel numerical method or perhaps something cool in the field of machine learning. Maybe you could build a encoder/decoder for an image format. Basically, you can CUDA-fy mostly anything compute intensive around you. I&rsquo;m not saying that <em>everything</em> is going to work well with CUDA - thats the topic for another blog post. But as someone starting out, one shouldn&rsquo;t be overly picky about that.</p>

<p>Oh, and theres always <a href="http://www.stackoverflow.com/questions/tagged/cuda">stackoverflow</a>, <a href="www.reddit.com/r/CUDA">/r/CUDA</a> and <a href="https://devtalk.nvidia.com/default/board/53/accelerated-computing/">NVIDIA&rsquo;s developer forum</a> if you get stuck somewhere - or even just want to discuss your ideas.</p>

<p>As with any new endeavor, you will fail and learn a lot. But the key as always is to persevere and accept experience that comes your way, whatever the form.</p>
]]></content>
  </entry>
  
</feed>
